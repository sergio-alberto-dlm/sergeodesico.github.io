---
title: 'Paper review: CLIP'
date: 2023-06-05
permalink: /posts/2023/06/blog-post-1/
tags:
 - Paper review
 - CV
 - NLP
---

In this entry we will review the paper: Learning Transferable Visual Models From Natural Language Supervision from OpenAI, a language-vision embedding model trained in a contrastive learning framework.

Motivation
======

It is a crucial question how to effectively combine language and vision modalities so that together outperforms their unimodal counterparts, in the other hand, as models get bigger and bigger make even slight modifications to them is highly computational expensive, so CLIP propose a novel approach where is possible to perform downstream tasks like classification without the need of modify the model architecture, as well as a training technique which is scalable.

How it works
======

The main idea behind the article is the natural language supervision, as the authors point it out, one advantage of this setting is that is scalable in contrast with the traditional supervision learning approach where we need annotated data, indeed at first, they try to predict the caption of an image, but this was difficult to scale. So they adopt the learning task of prediction which text description match with which image, as a result they observe 4x efficiency improvement on zero-shot transfer on ImageNet, in order to do this they first embed text and image in the same space and maximize the cosine similarity between real pairs while minimize on the rest of incorrect pairs. For the text encoder they used a Transformer with a max sequence length of 76 tokens and for the image encoder they used a ViT.

![CLIP Model Architecture](/images/pipeline_train_clip.png)

Data
======
The authors of this paper appeal to the vast amount of data publicly available on the internet so they decide to build a 400 million image-text pairs, the text used for training is comparable with the amount of text used to train GPT-2.

Results
======
So in order to perform zero-shot classification tasks on unseen datasets, we follow this pipeline, first we encode the possible labels in natural language with the text encoder, for instance “a photo of {object}” so we get the text embeddings, then we encode as well the image we want to classify and we take the nearest text embedding to the image as the prediction made by the model.

![Zero-Shot Classification Pipeline](/images/classification_clip.png)

The authors take as baseline a logistic regression model with the features of a ResNet-50 model and compare over 27 different datasets, achieving a considerable improvement with 16 of them. They realize that the model has serious limitations compare with more specialize datasets like KITTI Distance.

![Performance Comparison](/images/results_clip.png)

Why it matters?
======
CLIP is widely used for other models due to its power of combining text and images on the same space.

Takeaways and what we are thinking
======
- Contrastive learning 
- Language vision embeddings
- Text and image encoders
